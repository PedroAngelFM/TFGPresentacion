\subsection{Análisis factorial}
\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}

El objetivo principal del análisis factorial es estudiar la existencia de estructuras latentes en conjuntos de datos

Esto se traduce buscando si las variables aleatorias pueden ver su variabilidad explicada por un conjunto menor de factores comunes. 

Por tanto, lo que se busca es expresar la variabilidad del conjunto de datos en términos comunes con el resto y una parte específica. 
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
Supóngase un vector aleatorio $\mathbf{x}= [X_1, \ldots, X_p]$ de longitud $p$ con una distribución $N(0,\Sigma)$, centrada sin pérdida de generalidad, donde la matriz de covarianzas $\mathbf{\Sigma}$ es simétrica y definido positiva. El modelo factorial se establece:  
\begin{equation}\label{eq Fact}
X_j= \lambda_{1j}F_1+\ldots+\lambda_{mj}F_m+\psi_j U_j\quad j=1\ldots p 
\end{equation}
\noindent Donde:
\begin{itemize}
\item $\lambda_{kj}$ es la saturación de la $j$-ésima variable en el factor común $k$-ésimo. 
\item $F_k$ es el $k$-ésimo factor común
\item $\psi_j$ es la saturación  específica de la variable $X_j$ en el factor único. 
\item $U_j$ es el factor específico para la variable $X_j$.
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
\noindent El análisis factorial se apoya en las siguientes hipótesis :
\begin{itemize}
\item Los factores comunes $F_k$ son variables aleatorias que siguen una distribución marginal $N(0,1)$. Además se supondrá que $Cov(F_k,F_{k'})=0, k\neq k', \forall k,k'=1,\ldots, m$. Se suponen completamente independientes de los factores específicos. 

\item Los factores específicos $U_j$ son variables aleatorias con una distribución normal $N(0,1)$ no correladas. Se suponen  completamente independientes de los factores comunes. 
\end{itemize} 

\end{column}
\end{columns}
\end{frame}


\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
\begin{defi}
Llamaremos matriz de saturaciones de los factores o matriz de saturaciones, $\mathbf{\Lambda}$ de tamaño $p \times m$ a la matriz :
\begin{equation}
\mathbf{\Lambda}=\begin{pmatrix}
\lambda_{11} & \cdots & \lambda_{1 m}\\
\vdots & \ddots & \vdots\\
\lambda_{p1} & \cdots & \lambda_{pm}
\end{pmatrix}
\end{equation}
\end{defi}
\begin{defi}
Se llama comunalidad de la variable $X_j$, a $h_j^2=\sum_{k=1}^m \lambda_{kj}^2 $ .
\end{defi} 
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
\begin{defi}
Llamaremos matriz específica a la matriz diagonal $\mathbf{\Psi}$ de tamaño $p\times p$ a aquella que contiene los términos $\psi_j$ donde $j=1,\ldots , p$:
\begin{equation}
\mathbf{\Psi}=\begin{pmatrix}
    \psi_1 & 0 & \dots & 0 \\
    0 & \psi_2 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \psi_p
\end{pmatrix}
\end{equation}
\end{defi}
\begin{defi}
Se llama especificidad o unicidad de la variable $X_j$ a $\psi_j^2, \forall j=1,\ldots, p$.
\end{defi}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
\begin{propo}
La varianza de la variable aleatoria $X_j$, $\sigma_j^2$, se puede descomponer de la siguiente manera:
\begin{equation}
\sigma_j^2 = \sum_{k=1}^{m}\lambda_{kj}^2+\psi_j^2\quad \forall j=1,\ldots,p
\end{equation}
\end{propo}
\begin{propo}
La covarianza de dos variables $Cov(X_j, X_{j'})$, $\sigma_{jj'}$ cumple que 
\begin{equation}
\sigma_{jj'}=\sigma_{j'j}=\sum_{k=1}^{m}\lambda_{kj}\lambda_{kj'}
\end{equation}
\end{propo}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
\begin{teorema}\label{Descomposición Varianza}
La matriz $\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^T+\mathbf{\Psi}^2$
\end{teorema}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
Para la estimación de la matriz de satuaciones hay que tener en cuenta la siguiente descomposición:
\begin{equation}
\mathbf{S=\Lambda\Lambda}^T+\mathbf{\Psi}^2
\end{equation}

Por tanto, la matriz de saturaciones $\mathbf{\Lambda}$ se puede estimar haciendo la descomposición en valores y vectores propios de esta matriz. 
\begin{equation}
\mathbf{S-\Psi^2}=\mathbf{\Lambda\Lambda}^T=\mathbf{UDU}^T
\end{equation}

Si $\Psi=0$ se tiene la descomposición en valores y vectores propios de la matriz de covarianzas muestral. 
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
Para poder calcular simultáneamente la matriz de saturaciones la matriz específica se plantea el siguiente proceso iterativo. 
\noindent El método se inicia con $\mathbf{\hat{\Psi}}_0=0$ de tal manera que $\mathbf{S}_0^*=\mathbf{S}$. En cada iteración se efectúan las siguientes operaciones: 
\begin{enumerate}
\item Se calcula $\mathbf{S}_i^*=\mathbf{S-\hat{\Psi}}_i^2$.
\item Se calcula la descomposición $\mathbf{\hat{\Lambda}}_i$ como se ha detallado antes obteniendo $\mathbf{U}_i$ y $\mathbf{D}_i$ de tamaños $p\times m $ y $m\times m$. 
\item Se obtiene la nueva estimación de la $\mathbf{\hat{\Psi}}_{i+1}^2=\mathbf{S}-\mathbf{\hat{\Lambda}}_i\mathbf{\hat{\Lambda}}_i^T$
\end{enumerate}

\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}

El modelo está indeterminado por transformaciones ortogonales.
\begin{equation}
\mathbf{x}^T=(\mathbf{\Lambda T}^T)(\mathbf{Tf}^T)+\Psi\mathbf{u}^T
\end{equation}


Para evitar estas indeterminaciones se suelen imponer dos restricciones:
\begin{equation}
\mathbf{\Lambda}^T \mathbf{\Psi}^{-1}\mathbf{\Lambda} \text{ ó } \mathbf{\Lambda}^T \mathbf{\Lambda} \text{ son diagonales con los valores en orden decreciente }
\end{equation}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Métodos no supervisados}
\begin{columns}
\begin{column}{0.9\textwidth}
\noindent Para interpretar el modelo factorial hay que tener en cuenta los siguientes aspectos:
\begin{itemize}
\item Proporción de varianza común explicada.
\item Especificidad de cada una de las variables.
\item Saturaciones de cada variable en los factores.
\item Significado de los factores
\end{itemize}

\noindent Una vez interpretados los resultados, se puede llegar a los objetivos del análisis factorial, estudiar la estructura latente de los datos en términos de variabilidades comunes. 
\end{column}
\end{columns}
\end{frame}


